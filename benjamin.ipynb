{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Benjamin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "import pyarrow\n",
    "import bz2\n",
    "import json\n",
    "import datetime\n",
    "from iteration_utilities import deepflatten\n",
    "import nltk as nltk\n",
    "import scipy.stats as stats\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "pd.options.mode.chained_assignment = None\n",
    "from textblob import TextBlob\n",
    "from helpers import add_time, week, weekday_number, weekday, month, add_dict, to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/Benjamin/Desktop/Quotebank/'\n",
    "files = ['quotes-2015.json.bz2','quotes-2016.json.bz2','quotes-2017.json.bz2',\n",
    "                  'quotes-2018.json.bz2','quotes-2019.json.bz2','quotes-2020.json.bz2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1. dataset\n",
      "   date_short  compound                                     polarity  \\\n",
      "0  2015-08-31    0.8020    (0.43333333333333335, 0.4499999999999999)   \n",
      "1  2015-12-08    0.0000                                   (0.6, 1.0)   \n",
      "2  2015-09-10   -0.8720  (-0.17500000000000002, 0.30000000000000004)   \n",
      "3  2015-07-23    0.0000                   (0.16, 0.5399999999999999)   \n",
      "4  2015-10-04   -0.1779                                   (0.0, 0.0)   \n",
      "\n",
      "    weekday      Month  \n",
      "0    Monday     August  \n",
      "1   Tuesday   December  \n",
      "2  Thursday  September  \n",
      "3  Thursday       July  \n",
      "4    Sunday    October  \n"
     ]
    }
   ],
   "source": [
    "#defining sentiment analyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initializing dictionaries for dates, weekdays and months that will store values for all the chunks\n",
    "date_dictionary = {}\n",
    "\n",
    "# Dictionaries for weekdays and months are initialized with keys to keep them in the correct order for later plotting\n",
    "weekday_dictionary = {'Monday': 0, 'Tuesday': 0, 'Wednesday': 0, 'Thursday': 0,\n",
    "                      'Friday': 0, 'Saturday': 0, 'Sunday': 0}\n",
    "\n",
    "month_dictionary = {'January': 0, 'February': 0,'March': 0,\n",
    "                    'April': 0, 'May': 0, 'June': 0, \n",
    "                    'July': 0, 'August': 0, 'September': 0,\n",
    "                    'October': 0, 'November': 0, 'December': 0}\n",
    "\n",
    "# Iterating through all the cleaned data sets in chunks\n",
    "for x in range(len(files)): \n",
    "  print('Working on %d. dataset'%(x+1))\n",
    "  df_reader = pd.read_json(data_path + files[x], lines=True, compression='bz2', chunksize=50000)\n",
    "  for chunk in df_reader:\n",
    "    chunk = add_time(chunk)\n",
    "    \n",
    "    # Inserting values in the dictionaries\n",
    "    date_dictionary = to_dict(date_dictionary, chunk['date_short'])\n",
    "    weekday_dictionary = to_dict(weekday_dictionary, chunk['weekday'])\n",
    "    month_dictionary = to_dict(month_dictionary, chunk['Month'])\n",
    "    \n",
    "    chunk['compound'] = chunk['quotation'].apply(lambda x: sent_analyzer.polarity_scores(x)['compound'])\n",
    "    chunk['polarity'] = chunk['quotation'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "    chunk['subjectivity'] = chunk['quotation'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "    \n",
    "    chunk = chunk[['date_short','compound','polarity','subjectivity','weekday', 'Month']]\n",
    "    print(chunk.head())\n",
    "    break\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad4c0613312704d8cc10c140de42faceef6d81b6d49a9f4d806a60569ce36c6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ada': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
